{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8745ad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SNV data from AVPC/ACC55/55-post.snv.txt with shape (9952, 4)\n",
      "Loaded CNA data from AVPC/ACC55/55-post.cna.txt with shape (355, 5)\n",
      "Purity: 0.87\n",
      "Constructed df with shape (9952, 11)\n",
      "Loaded SNV data from AVPC/ACC55/55-pre.snv.txt with shape (8956, 4)\n",
      "Loaded CNA data from AVPC/ACC55/55-pre.cna.txt with shape (312, 5)\n",
      "Purity: 0.67\n",
      "Constructed df with shape (8956, 11)\n",
      "1376\n",
      "2372\n",
      "SNV file:      AVPC/ACC55pre/55post.snv.txt\n",
      "CNV file:      AVPC/ACC55pre/55post.cna.txt\n",
      "Purity file:   AVPC/ACC55pre/55post.purity.txt\n",
      "Sample ID:     sample_id\n",
      "Output dir:    E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc55post\n",
      "drop_data:     False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dropbox\\GitHub\\Multi_Region_CliPP\\clipp2\\preprocess.py:499: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  a3 = (actual_theta[No_w-1] - actual_theta[j]) / denom_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing. Results saved under: E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc55post\n",
      "Note: drop_data=False, so no rows were actually removed. All data are included in the output files.\n",
      "SNV file:      AVPC/ACC55pre/55pre.snv.txt\n",
      "CNV file:      AVPC/ACC55pre/55pre.cna.txt\n",
      "Purity file:   AVPC/ACC55pre/55pre.purity.txt\n",
      "Sample ID:     --sample_id\n",
      "Output dir:    E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc55pre\n",
      "drop_data:     False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dropbox\\GitHub\\Multi_Region_CliPP\\clipp2\\preprocess.py:499: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  a3 = (actual_theta[No_w-1] - actual_theta[j]) / denom_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing. Results saved under: E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc55pre\n",
      "Note: drop_data=False, so no rows were actually removed. All data are included in the output files.\n",
      "Loaded region 'acc55post': r.shape=(11328,), coef.shape=(11328, 6), purity=0.87\n",
      "Loaded region 'acc55pre': r.shape=(11328,), coef.shape=(11328, 6), purity=0.67\n",
      "\n",
      "=== Summary of grouped data before dropping rows ===\n",
      "Found M=2 regions. r shape= (11328, 2), n= (11328, 2)\n",
      "minor= (11328, 2), total= (11328, 2)\n",
      "coef_list length= 2 (each is (No_mutation,6))\n",
      "wcut= [-0.18  1.8 ]\n",
      "\n",
      "Dropped 0 rows that were all-zero in r/n/minor/total/coef.\n",
      "\n",
      "=== Summary of grouped data after dropping rows ===\n",
      "r shape= (11328, 2), n= (11328, 2)\n",
      "minor= (11328, 2), total= (11328, 2)\n",
      "coef_list length= 2, each => shape (11328, 6)\n"
     ]
    }
   ],
   "source": [
    "from clipp2.core_cuda import *\n",
    "from clipp2.preprocess import *\n",
    "df1 = process_files('AVPC/ACC55/55-post.snv.txt', 'AVPC/ACC55/55-post.cna.txt','AVPC/ACC55/55-post.purity.txt')\n",
    "df2 = process_files('AVPC/ACC55/55-pre.snv.txt', 'AVPC/ACC55/55-pre.cna.txt','AVPC/ACC55/55-pre.purity.txt')\n",
    "[df1, df2] = insert_distinct_rows_multi([df1, df2])\n",
    "export_snv_cna_and_purity(\n",
    "        df1,\n",
    "        dir=\"AVPC/ACC55pre/\",\n",
    "        snv_path=\"55post.snv.txt\",\n",
    "        cna_path=\"55post.cna.txt\",\n",
    "        purity_path=\"55post.purity.txt\"\n",
    "    )\n",
    "export_snv_cna_and_purity(\n",
    "        df2,\n",
    "        dir=\"AVPC/ACC55pre/\",\n",
    "        snv_path=\"55pre.snv.txt\",\n",
    "        cna_path=\"55pre.cna.txt\",\n",
    "        purity_path=\"55pre.purity.txt\"\n",
    "    )\n",
    "snv_file      = \"AVPC/ACC55pre/55post.snv.txt\"\n",
    "cn_file       = \"AVPC/ACC55pre/55post.cna.txt\"\n",
    "purity_file   = \"AVPC/ACC55pre/55post.purity.txt\"\n",
    "sample_id     = \"sample_id\"   # not heavily used in logic\n",
    "output_prefix = \"E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc55post\"\n",
    "\n",
    "# Call function with drop_data=True (to replicate R code's dropping logic)\n",
    "preprocess(\n",
    "    snv_file, cn_file, purity_file, sample_id, output_prefix, \n",
    "    drop_data=False\n",
    ")\n",
    "snv_file      = \"AVPC/ACC55pre/55pre.snv.txt\"\n",
    "cn_file       = \"AVPC/ACC55pre/55pre.cna.txt\"\n",
    "purity_file   = \"AVPC/ACC55pre/55pre.purity.txt\"\n",
    "sample_id     = \"--sample_id\"   # not heavily used in logic\n",
    "output_prefix = \"E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc55pre\"\n",
    "\n",
    "# Call function with drop_data=True (to replicate R code's dropping logic)\n",
    "preprocess(\n",
    "    snv_file, cn_file, purity_file, sample_id, output_prefix, \n",
    "    drop_data=False\n",
    ")\n",
    "root_dir = \"E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data\"\n",
    "(r, n, minor, total, purity, coef_list, wcut, drop) = group_all_regions_for_ADMM(root_dir)\n",
    "sample_size = np.shape(r)[0]\n",
    "sample_size_new = int(sample_size * 0.5)\n",
    "\n",
    "# Randomly choose 'sample_size' distinct row indices from [0..n-1]\n",
    "row_indices = np.random.choice(sample_size, size=sample_size_new, replace=False)\n",
    "\n",
    "r = r[row_indices, :]\n",
    "n = n[row_indices, :]\n",
    "minor = minor[row_indices, :]\n",
    "total = total[row_indices, :]\n",
    "\n",
    "coef_list = [x[row_indices, :] for x in coef_list]\n",
    "import pickle\n",
    "with open('rows.pkl', 'wb') as f:\n",
    "    pickle.dump(row_indices, f)\n",
    "\n",
    "wcut = [-1.8, 1.8]\n",
    "alpha=0.8\n",
    "gamma=3.7\n",
    "rho=1.02\n",
    "precision= 0.01\n",
    "Run_limit=20\n",
    "control_large=5\n",
    "Lambda=0.01\n",
    "post_th=0.05\n",
    "least_diff=0.01\n",
    "device='cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092c1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[np.isnan(r)] = 1\n",
    "n[np.isnan(n)] = 1000\n",
    "minor[np.isnan(minor)] = 1\n",
    "total[np.isnan(total)] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5c99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 1) Ensure shape (No_mutation, M) for all inputs --------\n",
    "def ensure_2D_and_no_zeros(arr):\n",
    "    \"\"\"\n",
    "    - If arr is 1D, reshape to (No_mutation, 1).\n",
    "    - If arr is 2D, keep shape.\n",
    "    - Then replace any zeros with 1.\n",
    "    \"\"\"\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1, 1)\n",
    "    elif arr.ndim != 2:\n",
    "        raise ValueError(f\"Expected 1D or 2D array, got shape {arr.shape}\")\n",
    "    # Replace zeros with 1\n",
    "    # This is vectorized, no Python loops:\n",
    "    arr = np.where(arr == 0, 1, arr)\n",
    "    return arr\n",
    "\n",
    "r     = ensure_2D_and_no_zeros(r)\n",
    "n     = ensure_2D_and_no_zeros(n)\n",
    "minor = ensure_2D_and_no_zeros(minor)\n",
    "total = ensure_2D_and_no_zeros(total)\n",
    "\n",
    "def to_torch_gpu(arr):\n",
    "    return torch.as_tensor(arr, dtype=torch.float32, device=device)\n",
    "\n",
    "r_t     = to_torch_gpu(r)\n",
    "n_t     = to_torch_gpu(n)\n",
    "minor_t = to_torch_gpu(minor)\n",
    "total_t = to_torch_gpu(total)\n",
    "\n",
    "# Build c_all => shape(No_mutation, M, 6)\n",
    "c_stack = []\n",
    "for c in coef_list:\n",
    "    # Each c => shape(No_mutation, 6)\n",
    "    c_stack.append(to_torch_gpu(c))\n",
    "c_all_t = torch.stack(c_stack, dim=1)  # shape => (No_mutation, M, 6)\n",
    "\n",
    "No_mutation, M = r_t.shape\n",
    "\n",
    "# 1) Prepare purity => shape (No_mutation, M)\n",
    "if isinstance(purity, (float, int)):\n",
    "    purity_t = torch.full((No_mutation, M), float(purity), device=device)\n",
    "else:\n",
    "    purity_vec = to_torch_gpu(purity)  # shape (M,)\n",
    "    purity_t   = purity_vec.unsqueeze(0).expand(No_mutation, -1)\n",
    "\n",
    "ploidy_t = torch.full((No_mutation, M), 2.0, device=device)\n",
    "\n",
    "# 2) Initialize w_new from logistic bounding\n",
    "fraction_t = r_t / (n_t + 1e-12)\n",
    "phi_hat_t  = fraction_t * ((ploidy_t - purity_t*ploidy_t) + (purity_t*total_t)) / (minor_t + 1e-12)\n",
    "\n",
    "scale_parameter = torch.clamp(torch.max(phi_hat_t), min=1.0)\n",
    "phi_new_t = phi_hat_t / scale_parameter\n",
    "\n",
    "low_b = torch.sigmoid(torch.tensor(-control_large, device=device))\n",
    "up_b  = torch.sigmoid(torch.tensor( control_large, device=device))\n",
    "phi_new_t = torch.clamp(phi_new_t, low_b, up_b)\n",
    "\n",
    "w_init_t = torch.log(phi_new_t) - torch.log(1 - phi_new_t + 1e-12)\n",
    "w_init_t = torch.clamp(w_init_t, -control_large, control_large)\n",
    "w_new_t  = w_init_t.clone()\n",
    "\n",
    "# 3) Build the sparse DELTA operator (COO)\n",
    "#    For i<j pairs, each row in the big (No_pairs*M) row-block has +1 at (i,m) and -1 at (j,m).\n",
    "#    We'll do this *without* for-loops using advanced indexing.\n",
    "i_idx_np, j_idx_np = torch.triu_indices(No_mutation, No_mutation, offset=1)\n",
    "# Move them to 'device'\n",
    "i_idx_np = i_idx_np.to(device)\n",
    "j_idx_np = j_idx_np.to(device)\n",
    "\n",
    "No_pairs = i_idx_np.size(0)\n",
    "\n",
    "# Then proceed to build row/col indices for the sparse Delta operator:\n",
    "pair_range = torch.arange(No_pairs, device=device)\n",
    "m_range    = torch.arange(M, device=device)\n",
    "\n",
    "# row_idx for the +1 block => shape (No_pairs, M)\n",
    "row_plus_2D = pair_range.unsqueeze(1)*M + m_range  # broadcast\n",
    "# row_idx for the -1 block => same row => row_plus_2D\n",
    "row_combined = torch.cat([row_plus_2D, row_plus_2D], dim=1)  # shape => (No_pairs, 2*M)\n",
    "\n",
    "# col_idx for +1 => i_idx_np*M + m\n",
    "col_plus_2D = i_idx_np.unsqueeze(1)*M + m_range\n",
    "# col_idx for -1 => j_idx_np*M + m\n",
    "col_minus_2D = j_idx_np.unsqueeze(1)*M + m_range\n",
    "col_combined = torch.cat([col_plus_2D, col_minus_2D], dim=1)\n",
    "\n",
    "row_idx = row_combined.reshape(-1)\n",
    "col_idx = col_combined.reshape(-1)\n",
    "\n",
    "# data => +1 then -1\n",
    "plus_block  = torch.ones_like(col_plus_2D, dtype=torch.float32)\n",
    "minus_block = -torch.ones_like(col_minus_2D, dtype=torch.float32)\n",
    "vals_2D = torch.cat([plus_block, minus_block], dim=1)\n",
    "vals = vals_2D.reshape(-1)\n",
    "\n",
    "total_rows = No_pairs*M\n",
    "total_cols = No_mutation*M\n",
    "Delta_coo = torch.sparse_coo_tensor(\n",
    "    indices=torch.stack([row_idx, col_idx], dim=0),\n",
    "    values=vals,\n",
    "    size=(total_rows, total_cols),\n",
    "    device=device\n",
    ").coalesce()\n",
    "\n",
    "# 4) Initialize eta, tau\n",
    "#    We can get the difference directly: D_w => shape(No_pairs, M) = w[i_idx]-w[j_idx]\n",
    "#    We'll re-use the code from scad_threshold_update for consistency.\n",
    "#    For initialization, just do:\n",
    "eta_new_t = (w_new_t[i_idx_np, :] - w_new_t[j_idx_np, :])\n",
    "tau_new_t = torch.ones_like(eta_new_t, device=device)\n",
    "\n",
    "# 5) ADMM iteration\n",
    "residual = 1e6\n",
    "k_iter   = 0\n",
    "\n",
    "low_cut, up_cut = wcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2627e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1, alpha=0.8160, residual=1.2625\n"
     ]
    }
   ],
   "source": [
    "# Might also stop if we get nan\n",
    "# (we do a small check below)\n",
    "k_iter += 1\n",
    "\n",
    "w_old_t  = w_new_t.clone()\n",
    "eta_old_t = eta_new_t.clone()\n",
    "tau_old_t = tau_new_t.clone()\n",
    "\n",
    "# =========== (A) IRLS expansions (vectorized) ===========\n",
    "expW_t = torch.exp(w_old_t)\n",
    "denom_ = 2.0 + expW_t*total_t\n",
    "denom_ = torch.clamp(denom_, min=1e-12)\n",
    "theta_t = (expW_t * minor_t)/denom_\n",
    "\n",
    "maskLow_t = (w_old_t <= low_cut)\n",
    "maskUp_t  = (w_old_t >= up_cut)\n",
    "maskMid_t = ~(maskLow_t | maskUp_t)\n",
    "\n",
    "# c_all_t => shape(No_mutation, M, 6)\n",
    "partA_full_t = (\n",
    "    maskLow_t * c_all_t[...,1]\n",
    "    + maskUp_t  * c_all_t[...,5]\n",
    "    + maskMid_t * c_all_t[...,3]\n",
    ") - (r_t/(n_t+1e-12))\n",
    "\n",
    "partB_full_t = (\n",
    "    maskLow_t * c_all_t[...,0]\n",
    "    + maskUp_t  * c_all_t[...,4]\n",
    "    + maskMid_t * c_all_t[...,2]\n",
    ")\n",
    "\n",
    "sqrt_n_t = torch.sqrt(n_t)\n",
    "denom2_t = torch.sqrt(theta_t*(1-theta_t) + 1e-12)\n",
    "\n",
    "A_array_t = (sqrt_n_t * partA_full_t)/denom2_t\n",
    "B_array_t = (sqrt_n_t * partB_full_t)/denom2_t\n",
    "\n",
    "A_flat_t = A_array_t.flatten()  # shape => (No_mutation*M,)\n",
    "B_flat_t = B_array_t.flatten()\n",
    "\n",
    "# ---------------- (B) Build system & solve with manual CG ----------------\n",
    "\n",
    "# 1) Get dimensions and build diag(B^2):\n",
    "NM = B_flat_t.shape[0]      # = (No_mutation * M)\n",
    "B_sq_t = B_flat_t**2        # shape => (NM,)\n",
    "\n",
    "# 2) Compute sparse Delta^T Delta => shape (NM, NM)\n",
    "Delta_t = Delta_coo.transpose(0, 1)  # shape => (NM, No_pairs*M)\n",
    "DTD = torch.sparse.mm(Delta_t, Delta_coo)  # shape => (NM, NM), still sparse\n",
    "\n",
    "# 3) Define a function that multiplies H = (diag(B^2) + alpha * (Delta^T Delta)) by x\n",
    "def matvec_H(x):\n",
    "    # x shape => (NM,)\n",
    "    # (A) multiply by diag(B_sq_t)\n",
    "    out = B_sq_t * x\n",
    "    # (B) add alpha * (DTD @ x)\n",
    "    out += alpha * torch.sparse.mm(DTD, x.unsqueeze(-1)).squeeze(-1)\n",
    "    return out\n",
    "\n",
    "# 4) Build the right-hand side: linear_t\n",
    "big_eta_tau_t = alpha*eta_old_t + tau_old_t   # shape => (No_pairs, M)\n",
    "big_eta_tau_flat_t = big_eta_tau_t.flatten()  # shape => (No_pairs*M,)\n",
    "\n",
    "RHS_1 = torch.sparse.mm(Delta_t, big_eta_tau_flat_t.unsqueeze(1)).squeeze(1)\n",
    "linear_t = RHS_1 - (B_flat_t * A_flat_t)      # shape => (NM,)\n",
    "\n",
    "# 5) Manual Conjugate Gradient solve\n",
    "#    We'll do a 'while True' to avoid Python 'for' loops.\n",
    "#    x0: initial guess => use the old solution w_old_t.\n",
    "x = w_old_t.flatten().clone()\n",
    "r = linear_t - matvec_H(x)\n",
    "p = r.clone()\n",
    "rs_old = torch.dot(r, r)\n",
    "\n",
    "max_cg_iter = 500\n",
    "tol = 1e-6\n",
    "iter_cg = 0\n",
    "\n",
    "while True:\n",
    "    # Ap = H * p\n",
    "    Ap = matvec_H(p)\n",
    "    denom_ = torch.dot(p, Ap) + 1e-12\n",
    "    alpha_cg = rs_old / denom_\n",
    "    \n",
    "    x = x + alpha_cg * p\n",
    "    r = r - alpha_cg * Ap\n",
    "    rs_new = torch.dot(r, r)\n",
    "\n",
    "    iter_cg += 1\n",
    "    # Check stopping criteria\n",
    "    if rs_new.sqrt() < tol or iter_cg >= max_cg_iter:\n",
    "        break\n",
    "\n",
    "    p = r + (rs_new / rs_old) * p\n",
    "    rs_old = rs_new\n",
    "\n",
    "# x now holds the solution for (diag(B^2) + alpha * DTD) * x = linear_t\n",
    "w_new_flat_t = x\n",
    "w_new_t = w_new_flat_t.view(No_mutation, M)\n",
    "\n",
    "# Finally, clamp in [-control_large, control_large]\n",
    "w_new_t = torch.clamp(w_new_t, -control_large, control_large)\n",
    "\n",
    "\n",
    "# Finally, clamp in [-control_large, control_large]\n",
    "w_new_t = torch.clamp(w_new_t, -control_large, control_large)\n",
    "\n",
    "# =========== (C) SCAD threshold ===========  \n",
    "eta_new_t, tau_new_t = scad_threshold_update_torch(\n",
    "    w_new_t, tau_old_t, Delta_coo, alpha, Lambda, gamma\n",
    ")\n",
    "\n",
    "# scale alpha\n",
    "alpha *= rho\n",
    "\n",
    "# =========== (D) residual check ===========\n",
    "# residual = max_{i<j,m} | (w[i]-w[j]) - eta[k]| \n",
    "# We'll do it again with the Delta approach\n",
    "# D_w = Delta_coo w_new_flat\n",
    "w_new_flat2 = w_new_t.flatten()\n",
    "D_w_flat2   = torch.sparse.mm(Delta_coo, w_new_flat2.unsqueeze(1)).squeeze(1)\n",
    "# reshape to (No_pairs, M)\n",
    "D_w2 = D_w_flat2.view(No_pairs, M)\n",
    "diff_2D_t = D_w2 - eta_new_t\n",
    "residual_val_t = torch.max(torch.abs(diff_2D_t))\n",
    "residual = float(residual_val_t.item())\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Iter={k_iter}, alpha={alpha:.4f}, residual={residual:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d114746",
   "metadata": {},
   "outputs": [],
   "source": [
    "while k_iter < Run_limit and residual > precision:\n",
    "\n",
    "    # Might also stop if we get nan\n",
    "    # (we do a small check below)\n",
    "    k_iter += 1\n",
    "\n",
    "    w_old_t  = w_new_t.clone()\n",
    "    eta_old_t = eta_new_t.clone()\n",
    "    tau_old_t = tau_new_t.clone()\n",
    "\n",
    "    # =========== (A) IRLS expansions (vectorized) ===========\n",
    "    expW_t = torch.exp(w_old_t)\n",
    "    denom_ = 2.0 + expW_t*total_t\n",
    "    denom_ = torch.clamp(denom_, min=1e-12)\n",
    "    theta_t = (expW_t * minor_t)/denom_\n",
    "\n",
    "    maskLow_t = (w_old_t <= low_cut)\n",
    "    maskUp_t  = (w_old_t >= up_cut)\n",
    "    maskMid_t = ~(maskLow_t | maskUp_t)\n",
    "\n",
    "    # c_all_t => shape(No_mutation, M, 6)\n",
    "    partA_full_t = (\n",
    "        maskLow_t * c_all_t[...,1]\n",
    "        + maskUp_t  * c_all_t[...,5]\n",
    "        + maskMid_t * c_all_t[...,3]\n",
    "    ) - (r_t/(n_t+1e-12))\n",
    "\n",
    "    partB_full_t = (\n",
    "        maskLow_t * c_all_t[...,0]\n",
    "        + maskUp_t  * c_all_t[...,4]\n",
    "        + maskMid_t * c_all_t[...,2]\n",
    "    )\n",
    "\n",
    "    sqrt_n_t = torch.sqrt(n_t)\n",
    "    denom2_t = torch.sqrt(theta_t*(1-theta_t) + 1e-12)\n",
    "\n",
    "    A_array_t = (sqrt_n_t * partA_full_t)/denom2_t\n",
    "    B_array_t = (sqrt_n_t * partB_full_t)/denom2_t\n",
    "\n",
    "    A_flat_t = A_array_t.flatten()  # shape => (No_mutation*M,)\n",
    "    B_flat_t = B_array_t.flatten()\n",
    "\n",
    "    # ---------------- (B) Build system & solve with manual CG ----------------\n",
    "\n",
    "    # 1) Get dimensions and build diag(B^2):\n",
    "    NM = B_flat_t.shape[0]      # = (No_mutation * M)\n",
    "    B_sq_t = B_flat_t**2        # shape => (NM,)\n",
    "\n",
    "    # 2) Compute sparse Delta^T Delta => shape (NM, NM)\n",
    "    Delta_t = Delta_coo.transpose(0, 1)  # shape => (NM, No_pairs*M)\n",
    "    DTD = torch.sparse.mm(Delta_t, Delta_coo)  # shape => (NM, NM), still sparse\n",
    "\n",
    "    # 3) Define a function that multiplies H = (diag(B^2) + alpha * (Delta^T Delta)) by x\n",
    "    def matvec_H(x):\n",
    "        # x shape => (NM,)\n",
    "        # (A) multiply by diag(B_sq_t)\n",
    "        out = B_sq_t * x\n",
    "        # (B) add alpha * (DTD @ x)\n",
    "        out += alpha * torch.sparse.mm(DTD, x.unsqueeze(-1)).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "    # 4) Build the right-hand side: linear_t\n",
    "    big_eta_tau_t = alpha*eta_old_t + tau_old_t   # shape => (No_pairs, M)\n",
    "    big_eta_tau_flat_t = big_eta_tau_t.flatten()  # shape => (No_pairs*M,)\n",
    "\n",
    "    RHS_1 = torch.sparse.mm(Delta_t, big_eta_tau_flat_t.unsqueeze(1)).squeeze(1)\n",
    "    linear_t = RHS_1 - (B_flat_t * A_flat_t)      # shape => (NM,)\n",
    "\n",
    "    # 5) Manual Conjugate Gradient solve\n",
    "    #    We'll do a 'while True' to avoid Python 'for' loops.\n",
    "    #    x0: initial guess => use the old solution w_old_t.\n",
    "    x = w_old_t.flatten().clone()\n",
    "    r = linear_t - matvec_H(x)\n",
    "    p = r.clone()\n",
    "    rs_old = torch.dot(r, r)\n",
    "\n",
    "    max_cg_iter = 500\n",
    "    tol = 1e-6\n",
    "    iter_cg = 0\n",
    "\n",
    "    while True:\n",
    "        # Ap = H * p\n",
    "        Ap = matvec_H(p)\n",
    "        denom_ = torch.dot(p, Ap) + 1e-12\n",
    "        alpha_cg = rs_old / denom_\n",
    "        \n",
    "        x = x + alpha_cg * p\n",
    "        r = r - alpha_cg * Ap\n",
    "        rs_new = torch.dot(r, r)\n",
    "\n",
    "        iter_cg += 1\n",
    "        # Check stopping criteria\n",
    "        if rs_new.sqrt() < tol or iter_cg >= max_cg_iter:\n",
    "            break\n",
    "\n",
    "        p = r + (rs_new / rs_old) * p\n",
    "        rs_old = rs_new\n",
    "\n",
    "    # x now holds the solution for (diag(B^2) + alpha * DTD) * x = linear_t\n",
    "    w_new_flat_t = x\n",
    "    w_new_t = w_new_flat_t.view(No_mutation, M)\n",
    "\n",
    "    # Finally, clamp in [-control_large, control_large]\n",
    "    w_new_t = torch.clamp(w_new_t, -control_large, control_large)\n",
    "\n",
    "\n",
    "    # Finally, clamp in [-control_large, control_large]\n",
    "    w_new_t = torch.clamp(w_new_t, -control_large, control_large)\n",
    "\n",
    "    # =========== (C) SCAD threshold ===========  \n",
    "    eta_new_t, tau_new_t = scad_threshold_update_torch(\n",
    "        w_new_t, tau_old_t, Delta_coo, alpha, Lambda, gamma\n",
    "    )\n",
    "\n",
    "    # scale alpha\n",
    "    alpha *= rho\n",
    "\n",
    "    # =========== (D) residual check ===========\n",
    "    # residual = max_{i<j,m} | (w[i]-w[j]) - eta[k]| \n",
    "    # We'll do it again with the Delta approach\n",
    "    # D_w = Delta_coo w_new_flat\n",
    "    w_new_flat2 = w_new_t.flatten()\n",
    "    D_w_flat2   = torch.sparse.mm(Delta_coo, w_new_flat2.unsqueeze(1)).squeeze(1)\n",
    "    # reshape to (No_pairs, M)\n",
    "    D_w2 = D_w_flat2.view(No_pairs, M)\n",
    "    diff_2D_t = D_w2 - eta_new_t\n",
    "    residual_val_t = torch.max(torch.abs(diff_2D_t))\n",
    "    residual = float(residual_val_t.item())\n",
    "\n",
    "    if torch.isnan(residual_val_t):\n",
    "        break\n",
    "\n",
    "    print(f\"Iter={k_iter}, alpha={alpha:.4f}, residual={residual:.6g}\")\n",
    "\n",
    "print(\"\\nADMM finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_new = w_new_t.detach().cpu().numpy()\n",
    "eta_new = eta_new_t.detach().cpu().numpy()\n",
    "phi_hat = phi_hat_t.detach().cpu().numpy()\n",
    "diff = diff_mat(w_new)\n",
    "ids = np.triu_indices(diff.shape[1], 1)\n",
    "eta_new[np.where(np.abs(eta_new) <= post_th)] = 0\n",
    "diff[ids] = np.linalg.norm(eta_new, axis=1)\n",
    "class_label = -np.ones(No_mutation)\n",
    "class_label[0] = 0\n",
    "group_size = [1]\n",
    "labl = 1\n",
    "\n",
    "for i in range(1, No_mutation):\n",
    "    for j in range(i):\n",
    "        if diff[j, i] == 0:\n",
    "            class_label[i] = class_label[j]\n",
    "            group_size[int(class_label[j])] += 1\n",
    "            break\n",
    "    if class_label[i] == -1:\n",
    "        class_label[i] = labl\n",
    "        labl += 1\n",
    "        group_size.append(1)\n",
    "\n",
    "# quality control\n",
    "least_mut = np.ceil(0.05 * No_mutation)\n",
    "tmp_size = np.min(np.array(group_size)[np.array(group_size) > 0])\n",
    "tmp_grp = np.where(group_size == tmp_size)\n",
    "refine = False\n",
    "if tmp_size < least_mut:\n",
    "    refine = True\n",
    "\n",
    "while refine:\n",
    "    refine = False\n",
    "    tmp_col = np.where(class_label == tmp_grp[0][0])[0]\n",
    "    for i in range(len(tmp_col)):\n",
    "        if tmp_col[i] != 0 and tmp_col[i] != No_mutation - 1:\n",
    "            tmp_diff = np.abs(np.append(np.append(diff[0:tmp_col[i], tmp_col[i]].T.ravel(), 100),\n",
    "                                        diff[tmp_col[i], (tmp_col[i] + 1):No_mutation].ravel()))\n",
    "            tmp_diff[tmp_col] += 100\n",
    "            diff[0:tmp_col[i], tmp_col[i]] = tmp_diff[0:tmp_col[i]]\n",
    "            diff[tmp_col[i], (tmp_col[i] + 1):No_mutation] = tmp_diff[(tmp_col[i] + 1):No_mutation]\n",
    "        elif tmp_col[i] == 0:\n",
    "            tmp_diff = np.append(100, diff[0, 1:No_mutation])\n",
    "            tmp_diff[tmp_col] += 100\n",
    "            diff[0, 1:No_mutation] = tmp_diff[1:No_mutation]\n",
    "        else:\n",
    "            tmp_diff = np.append(diff[0:(No_mutation - 1), No_mutation - 1], 100)\n",
    "            tmp_diff[tmp_col] += 100\n",
    "            diff[0:(No_mutation - 1), No_mutation - 1] = tmp_diff[0:(No_mutation - 1)]\n",
    "        ind = tmp_diff.argmin()\n",
    "        group_size[class_label.astype(np.int64, copy=False)[tmp_col[i]]] -= 1\n",
    "        class_label[tmp_col[i]] = class_label[ind]\n",
    "        group_size[class_label.astype(np.int64, copy=False)[tmp_col[i]]] += 1\n",
    "    tmp_size = np.min(np.array(group_size)[np.array(group_size) > 0])\n",
    "    tmp_grp = np.where(group_size == tmp_size)\n",
    "    refine = False\n",
    "    if tmp_size < least_mut:\n",
    "        refine = True\n",
    "\n",
    "labels = np.unique(class_label)\n",
    "phi_out = np.zeros((len(labels), M))\n",
    "for i in range(len(labels)):\n",
    "    ind = np.where(class_label == labels[i])[0]\n",
    "    class_label[ind] = i\n",
    "    phi_out[i, :] = np.sum(phi_hat[ind, : ] * n[ind, : ], axis=0) / np.sum(n[ind, : ], axis=0)\n",
    "\n",
    "if len(labels) > 1:\n",
    "    sort_phi = sort_by_2norm(phi_out)\n",
    "    phi_diff = sort_phi[1:, :] - sort_phi[:-1, :]\n",
    "    min_ind, min_val = find_min_row_by_2norm(phi_diff)\n",
    "    while np.linalg.norm(min_val) < least_diff:\n",
    "        combine_ind = np.where(phi_out == sort_phi[min_ind, ])[0]\n",
    "        combine_to_ind = np.where(phi_out == sort_phi[min_ind + 1, ])[0]\n",
    "        class_label[class_label == combine_ind] = combine_to_ind\n",
    "        labels = np.unique(class_label)\n",
    "        phi_out = np.zeros((len(labels), M))\n",
    "        for i in range(len(labels)):\n",
    "            ind = np.where(class_label == labels[i])[0]\n",
    "            class_label[ind] = i\n",
    "            phi_out[i] = np.sum(phi_hat[ind, : ] * n[ind, : ], axis=0) / np.sum(n[ind, : ], axis=0)\n",
    "        if len(labels) == 1:\n",
    "            break\n",
    "        else:\n",
    "            sort_phi = sort_by_2norm(phi_out)\n",
    "            phi_diff = sort_phi[1:, :] - sort_phi[:-1, :]\n",
    "            min_ind, min_val = find_min_row_by_2norm(phi_diff)\n",
    "phi_res = np.zeros((No_mutation, M))\n",
    "for lab in range(np.shape(phi_out)[0]):\n",
    "    phi_res[class_label == lab, ] = phi_out[lab, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b759d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc3918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a7d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde9833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
