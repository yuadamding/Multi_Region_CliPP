{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8745ad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SNV data from AVPC/ACC9/9-post.snv.txt with shape (3862, 4)\n",
      "Loaded CNA data from AVPC/ACC9/9-post.cna.txt with shape (181, 6)\n",
      "Purity: 0.21\n",
      "Constructed df with shape (3862, 11)\n",
      "Warning: No match in CNA for row 2269 but found major_cn=2.0 and minor_cn=2.0. Expected defaults 1 and 1.\n",
      "Warning: No match in CNA for row 2001 but found major_cn=6.0 and minor_cn=2.0. Expected defaults 1 and 1.\n",
      "Loaded SNV data from AVPC/ACC9/9-pre.snv.txt with shape (5576, 4)\n",
      "Loaded CNA data from AVPC/ACC9/9-pre.cna.txt with shape (277, 6)\n",
      "Purity: 0.31\n",
      "Constructed df with shape (5576, 11)\n",
      "Warning: No match in CNA for row 2684 but found major_cn=2.0 and minor_cn=2.0. Expected defaults 1 and 1.\n",
      "Warning: No match in CNA for row 1114 but found major_cn=4.0 and minor_cn=1.0. Expected defaults 1 and 1.\n",
      "Warning: No match in CNA for row 3031 but found major_cn=38.0 and minor_cn=2.0. Expected defaults 1 and 1.\n",
      "Warning: No match in CNA for row 2054 but found major_cn=4.0 and minor_cn=2.0. Expected defaults 1 and 1.\n",
      "Warning: No match in CNA for row 5555 but found major_cn=2.0 and minor_cn=1.0. Expected defaults 1 and 1.\n",
      "Process done. Created outputs in E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc9post.\n"
     ]
    }
   ],
   "source": [
    "from clipp2.core_cuda import *\n",
    "from clipp2.preprocess import *\n",
    "df1 = process_files('AVPC/ACC9/9-post.snv.txt', 'AVPC/ACC9/9-post.cna.txt','AVPC/ACC9/9-post.purity.txt')\n",
    "df2 = process_files('AVPC/ACC9/9-pre.snv.txt', 'AVPC/ACC9/9-pre.cna.txt','AVPC/ACC9/9-pre.purity.txt')\n",
    "[df1, df2] = insert_distinct_rows_multi([df1, df2])\n",
    "export_snv_cna_and_purity(\n",
    "        df1,\n",
    "        dir=\"AVPC/ACC9pre/\",\n",
    "        snv_path=\"9post.snv.txt\",\n",
    "        cna_path=\"9post.cna.txt\",\n",
    "        purity_path=\"9post.purity.txt\"\n",
    "    )\n",
    "export_snv_cna_and_purity(\n",
    "        df2,\n",
    "        dir=\"AVPC/ACC9pre/\",\n",
    "        snv_path=\"9pre.snv.txt\",\n",
    "        cna_path=\"9pre.cna.txt\",\n",
    "        purity_path=\"9pre.purity.txt\"\n",
    "    )\n",
    "snv_file      = \"AVPC/ACC9pre/9post.snv.txt\"\n",
    "cn_file       = \"AVPC/ACC9pre/9post.cna.txt\"\n",
    "purity_file   = \"AVPC/ACC9pre/9post.purity.txt\"\n",
    "sample_id     = \"--sample_id\"   # not heavily used in logic\n",
    "output_prefix = \"E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc9post\"\n",
    "\n",
    "# Call function with drop_data=True (to replicate R code's dropping logic)\n",
    "process_data(\n",
    "    snv_file, cn_file, purity_file, sample_id, output_prefix, \n",
    "    drop_data=False\n",
    ")\n",
    "snv_file      = \"AVPC/ACC9pre/9pre.snv.txt\"\n",
    "cn_file       = \"AVPC/ACC9pre/9pre.cna.txt\"\n",
    "purity_file   = \"AVPC/ACC9pre/9pre.purity.txt\"\n",
    "sample_id     = \"--sample_id\"   # not heavily used in logic\n",
    "output_prefix = \"E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc9pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a6fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done. Created outputs in E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data/acc9pre.\n",
      "Loaded region 'acc9post': r.shape=(5793,), coef.shape=(5793, 6), purity=0.21\n",
      "Loaded region 'acc9pre': r.shape=(5793,), coef.shape=(5793, 6), purity=0.31\n",
      "\n",
      "=== Summary of grouped data before dropping rows ===\n",
      "Found M=2 regions. r shape= (5793, 2), n= (5793, 2)\n",
      "minor= (5793, 2), total= (5793, 2)\n",
      "coef_list length= 2 (each is (No_mutation,6))\n",
      "wcut= [-0.18  1.8 ]\n",
      "\n",
      "Dropped 0 rows that were all-zero in r/n/minor/total/coef.\n",
      "\n",
      "=== Summary of grouped data after dropping rows ===\n",
      "r shape= (5793, 2), n= (5793, 2)\n",
      "minor= (5793, 2), total= (5793, 2)\n",
      "coef_list length= 2, each => shape (5793, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dropbox\\GitHub\\Multi_Region_CliPP\\clipp2\\preprocess.py:763: RuntimeWarning: divide by zero encountered in divide\n",
      "  phi_arr = 2.0 / ( (minor_count / (minor_read/total_read)) - total_count + 2.0 )\n"
     ]
    }
   ],
   "source": [
    "# Call function with drop_data=True (to replicate R code's dropping logic)\n",
    "process_data(\n",
    "    snv_file, cn_file, purity_file, sample_id, output_prefix, \n",
    "    drop_data=False\n",
    ")\n",
    "root_dir = \"E:/Dropbox/GitHub/Multi_Region_CliPP/processed_data\"\n",
    "(r, n, minor, total, purity, coef_list, wcut, drop) = group_all_regions_for_ADMM(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d69e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcut = [-1.8, 1.8]\n",
    "alpha=0.8\n",
    "gamma=3.7\n",
    "rho=1.02\n",
    "precision= 0.01\n",
    "Run_limit=1e4\n",
    "control_large=5\n",
    "Lambda=0.1\n",
    "post_th=0.05\n",
    "least_diff=0.01\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5930910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_2D_and_no_zeros(arr):\n",
    "    \"\"\"\n",
    "    - If arr is 1D, reshape to (No_mutation, 1).\n",
    "    - If arr is 2D, keep shape.\n",
    "    - Then replace any zeros with 1.\n",
    "    \"\"\"\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1, 1)\n",
    "    elif arr.ndim != 2:\n",
    "        raise ValueError(f\"Expected 1D or 2D array, got shape {arr.shape}\")\n",
    "    # Replace zeros with 1\n",
    "    # This is vectorized, no Python loops:\n",
    "    arr = np.where(arr == 0, 1, arr)\n",
    "    return arr\n",
    "\n",
    "r     = ensure_2D_and_no_zeros(r)\n",
    "n     = ensure_2D_and_no_zeros(n)\n",
    "minor = ensure_2D_and_no_zeros(minor)\n",
    "total = ensure_2D_and_no_zeros(total)\n",
    "\n",
    "def to_torch_gpu(arr):\n",
    "    return torch.as_tensor(arr, dtype=torch.float32, device=device)\n",
    "\n",
    "r_t     = to_torch_gpu(r)\n",
    "n_t     = to_torch_gpu(n)\n",
    "minor_t = to_torch_gpu(minor)\n",
    "total_t = to_torch_gpu(total)\n",
    "\n",
    "# Build c_all => shape(No_mutation, M, 6)\n",
    "c_stack = []\n",
    "for c in coef_list:\n",
    "    # Each c => shape(No_mutation, 6)\n",
    "    c_stack.append(to_torch_gpu(c))\n",
    "c_all_t = torch.stack(c_stack, dim=1)  # shape => (No_mutation, M, 6)\n",
    "\n",
    "No_mutation, M = r_t.shape\n",
    "\n",
    "# 1) Prepare purity => shape (No_mutation, M)\n",
    "if isinstance(purity, (float, int)):\n",
    "    purity_t = torch.full((No_mutation, M), float(purity), device=device)\n",
    "else:\n",
    "    purity_vec = to_torch_gpu(purity)  # shape (M,)\n",
    "    purity_t   = purity_vec.unsqueeze(0).expand(No_mutation, -1)\n",
    "\n",
    "ploidy_t = torch.full((No_mutation, M), 2.0, device=device)\n",
    "\n",
    "# 2) Initialize w_new from logistic bounding\n",
    "fraction_t = r_t / (n_t + 1e-12)\n",
    "phi_hat_t  = fraction_t * ((ploidy_t - purity_t*ploidy_t) + (purity_t*total_t)) / (minor_t + 1e-12)\n",
    "\n",
    "scale_parameter = torch.clamp(torch.max(phi_hat_t), min=1.0)\n",
    "phi_new_t = phi_hat_t / scale_parameter\n",
    "\n",
    "low_b = torch.sigmoid(torch.tensor(-control_large, device=device))\n",
    "up_b  = torch.sigmoid(torch.tensor( control_large, device=device))\n",
    "phi_new_t = torch.clamp(phi_new_t, low_b, up_b)\n",
    "\n",
    "w_init_t = torch.log(phi_new_t) - torch.log(1 - phi_new_t + 1e-12)\n",
    "w_init_t = torch.clamp(w_init_t, -control_large, control_large)\n",
    "w_new_t  = w_init_t.clone()\n",
    "\n",
    "# 3) Build the sparse DELTA operator (COO)\n",
    "#    For i<j pairs, each row in the big (No_pairs*M) row-block has +1 at (i,m) and -1 at (j,m).\n",
    "#    We'll do this *without* for-loops using advanced indexing.\n",
    "i_idx_np, j_idx_np = torch.triu_indices(No_mutation, No_mutation, offset=1)\n",
    "# Move them to 'device'\n",
    "i_idx_np = i_idx_np.to(device)\n",
    "j_idx_np = j_idx_np.to(device)\n",
    "\n",
    "No_pairs = i_idx_np.size(0)\n",
    "\n",
    "# Then proceed to build row/col indices for the sparse Delta operator:\n",
    "pair_range = torch.arange(No_pairs, device=device)\n",
    "m_range    = torch.arange(M, device=device)\n",
    "\n",
    "# row_idx for the +1 block => shape (No_pairs, M)\n",
    "row_plus_2D = pair_range.unsqueeze(1)*M + m_range  # broadcast\n",
    "# row_idx for the -1 block => same row => row_plus_2D\n",
    "row_combined = torch.cat([row_plus_2D, row_plus_2D], dim=1)  # shape => (No_pairs, 2*M)\n",
    "\n",
    "# col_idx for +1 => i_idx_np*M + m\n",
    "col_plus_2D = i_idx_np.unsqueeze(1)*M + m_range\n",
    "# col_idx for -1 => j_idx_np*M + m\n",
    "col_minus_2D = j_idx_np.unsqueeze(1)*M + m_range\n",
    "col_combined = torch.cat([col_plus_2D, col_minus_2D], dim=1)\n",
    "\n",
    "row_idx = row_combined.reshape(-1)\n",
    "col_idx = col_combined.reshape(-1)\n",
    "\n",
    "# data => +1 then -1\n",
    "plus_block  = torch.ones_like(col_plus_2D, dtype=torch.float32)\n",
    "minus_block = -torch.ones_like(col_minus_2D, dtype=torch.float32)\n",
    "vals_2D = torch.cat([plus_block, minus_block], dim=1)\n",
    "vals = vals_2D.reshape(-1)\n",
    "\n",
    "total_rows = No_pairs*M\n",
    "total_cols = No_mutation*M\n",
    "Delta_coo = torch.sparse_coo_tensor(\n",
    "    indices=torch.stack([row_idx, col_idx], dim=0),\n",
    "    values=vals,\n",
    "    size=(total_rows, total_cols),\n",
    "    device=device\n",
    ").coalesce()\n",
    "\n",
    "# 4) Initialize eta, tau\n",
    "#    We can get the difference directly: D_w => shape(No_pairs, M) = w[i_idx]-w[j_idx]\n",
    "#    We'll re-use the code from scad_threshold_update for consistency.\n",
    "#    For initialization, just do:\n",
    "eta_new_t = (w_new_t[i_idx_np, :] - w_new_t[j_idx_np, :])\n",
    "tau_new_t = torch.ones_like(eta_new_t, device=device)\n",
    "\n",
    "# 5) ADMM iteration\n",
    "residual = 1e6\n",
    "k_iter   = 0\n",
    "\n",
    "low_cut, up_cut = wcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0257831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=1, alpha=0.8160, residual=1.375\n"
     ]
    }
   ],
   "source": [
    "while k_iter < Run_limit and residual > precision:\n",
    "\n",
    "    # Might also stop if we get nan\n",
    "    # (we do a small check below)\n",
    "    k_iter += 1\n",
    "\n",
    "    w_old_t  = w_new_t.clone()\n",
    "    eta_old_t = eta_new_t.clone()\n",
    "    tau_old_t = tau_new_t.clone()\n",
    "\n",
    "    # =========== (A) IRLS expansions (vectorized) ===========\n",
    "    expW_t = torch.exp(w_old_t)\n",
    "    denom_ = 2.0 + expW_t*total_t\n",
    "    denom_ = torch.clamp(denom_, min=1e-12)\n",
    "    theta_t = (expW_t * minor_t)/denom_\n",
    "\n",
    "    maskLow_t = (w_old_t <= low_cut)\n",
    "    maskUp_t  = (w_old_t >= up_cut)\n",
    "    maskMid_t = ~(maskLow_t | maskUp_t)\n",
    "\n",
    "    # c_all_t => shape(No_mutation, M, 6)\n",
    "    partA_full_t = (\n",
    "        maskLow_t * c_all_t[...,1]\n",
    "        + maskUp_t  * c_all_t[...,5]\n",
    "        + maskMid_t * c_all_t[...,3]\n",
    "    ) - (r_t/(n_t+1e-12))\n",
    "\n",
    "    partB_full_t = (\n",
    "        maskLow_t * c_all_t[...,0]\n",
    "        + maskUp_t  * c_all_t[...,4]\n",
    "        + maskMid_t * c_all_t[...,2]\n",
    "    )\n",
    "\n",
    "    sqrt_n_t = torch.sqrt(n_t)\n",
    "    denom2_t = torch.sqrt(theta_t*(1-theta_t) + 1e-12)\n",
    "\n",
    "    A_array_t = (sqrt_n_t * partA_full_t)/denom2_t\n",
    "    B_array_t = (sqrt_n_t * partB_full_t)/denom2_t\n",
    "\n",
    "    A_flat_t = A_array_t.flatten()  # shape => (No_mutation*M,)\n",
    "    B_flat_t = B_array_t.flatten()\n",
    "\n",
    "    # ---------------- (B) Build system & solve with manual CG ----------------\n",
    "\n",
    "    # 1) Get dimensions and build diag(B^2):\n",
    "    NM = B_flat_t.shape[0]      # = (No_mutation * M)\n",
    "    B_sq_t = B_flat_t**2        # shape => (NM,)\n",
    "\n",
    "    # 2) Compute sparse Delta^T Delta => shape (NM, NM)\n",
    "    Delta_t = Delta_coo.transpose(0, 1)  # shape => (NM, No_pairs*M)\n",
    "    DTD = torch.sparse.mm(Delta_t, Delta_coo)  # shape => (NM, NM), still sparse\n",
    "\n",
    "    # 3) Define a function that multiplies H = (diag(B^2) + alpha * (Delta^T Delta)) by x\n",
    "    def matvec_H(x):\n",
    "        # x shape => (NM,)\n",
    "        # (A) multiply by diag(B_sq_t)\n",
    "        out = B_sq_t * x\n",
    "        # (B) add alpha * (DTD @ x)\n",
    "        out += alpha * torch.sparse.mm(DTD, x.unsqueeze(-1)).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "    # 4) Build the right-hand side: linear_t\n",
    "    big_eta_tau_t = alpha*eta_old_t + tau_old_t   # shape => (No_pairs, M)\n",
    "    big_eta_tau_flat_t = big_eta_tau_t.flatten()  # shape => (No_pairs*M,)\n",
    "\n",
    "    RHS_1 = torch.sparse.mm(Delta_t, big_eta_tau_flat_t.unsqueeze(1)).squeeze(1)\n",
    "    linear_t = RHS_1 - (B_flat_t * A_flat_t)      # shape => (NM,)\n",
    "\n",
    "    # 5) Manual Conjugate Gradient solve\n",
    "    #    We'll do a 'while True' to avoid Python 'for' loops.\n",
    "    #    x0: initial guess => use the old solution w_old_t.\n",
    "    x = w_old_t.flatten().clone()\n",
    "    r = linear_t - matvec_H(x)\n",
    "    p = r.clone()\n",
    "    rs_old = torch.dot(r, r)\n",
    "\n",
    "    max_cg_iter = 500\n",
    "    tol = 1e-6\n",
    "    iter_cg = 0\n",
    "\n",
    "    while True:\n",
    "        # Ap = H * p\n",
    "        Ap = matvec_H(p)\n",
    "        denom_ = torch.dot(p, Ap) + 1e-12\n",
    "        alpha_cg = rs_old / denom_\n",
    "        \n",
    "        x = x + alpha_cg * p\n",
    "        r = r - alpha_cg * Ap\n",
    "        rs_new = torch.dot(r, r)\n",
    "\n",
    "        iter_cg += 1\n",
    "        # Check stopping criteria\n",
    "        if rs_new.sqrt() < tol or iter_cg >= max_cg_iter:\n",
    "            break\n",
    "\n",
    "        p = r + (rs_new / rs_old) * p\n",
    "        rs_old = rs_new\n",
    "\n",
    "    # x now holds the solution for (diag(B^2) + alpha * DTD) * x = linear_t\n",
    "    w_new_flat_t = x\n",
    "    w_new_t = w_new_flat_t.view(No_mutation, M)\n",
    "\n",
    "    # Finally, clamp in [-control_large, control_large]\n",
    "    w_new_t = torch.clamp(w_new_t, -control_large, control_large)\n",
    "\n",
    "\n",
    "    # Finally, clamp in [-control_large, control_large]\n",
    "    w_new_t = torch.clamp(w_new_t, -control_large, control_large)\n",
    "\n",
    "    # =========== (C) SCAD threshold ===========  \n",
    "    eta_new_t, tau_new_t = scad_threshold_update_torch(\n",
    "        w_new_t, tau_old_t, Delta_coo, alpha, Lambda, gamma\n",
    "    )\n",
    "\n",
    "    # scale alpha\n",
    "    alpha *= rho\n",
    "\n",
    "    # =========== (D) residual check ===========\n",
    "    # residual = max_{i<j,m} | (w[i]-w[j]) - eta[k]| \n",
    "    # We'll do it again with the Delta approach\n",
    "    # D_w = Delta_coo w_new_flat\n",
    "    w_new_flat2 = w_new_t.flatten()\n",
    "    D_w_flat2   = torch.sparse.mm(Delta_coo, w_new_flat2.unsqueeze(1)).squeeze(1)\n",
    "    # reshape to (No_pairs, M)\n",
    "    D_w2 = D_w_flat2.view(No_pairs, M)\n",
    "    diff_2D_t = D_w2 - eta_new_t\n",
    "    residual_val_t = torch.max(torch.abs(diff_2D_t))\n",
    "    residual = float(residual_val_t.item())\n",
    "\n",
    "    if torch.isnan(residual_val_t):\n",
    "        break\n",
    "\n",
    "    print(f\"Iter={k_iter}, alpha={alpha:.4f}, residual={residual:.6g}\")\n",
    "\n",
    "print(\"\\nADMM finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14168a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_new_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w_new \u001b[38;5;241m=\u001b[39m \u001b[43mw_new_t\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      2\u001b[0m eta_new \u001b[38;5;241m=\u001b[39m eta_new_t\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      3\u001b[0m phi_hat \u001b[38;5;241m=\u001b[39m phi_hat_t\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w_new_t' is not defined"
     ]
    }
   ],
   "source": [
    "w_new = w_new_t.detach().cpu().numpy()\n",
    "eta_new = eta_new_t.detach().cpu().numpy()\n",
    "phi_hat = phi_hat_t.detach().cpu().numpy()\n",
    "diff = diff_mat(w_new)\n",
    "ids = np.triu_indices(diff.shape[1], 1)\n",
    "eta_new[np.where(np.abs(eta_new) <= post_th)] = 0\n",
    "diff[ids] = np.linalg.norm(eta_new, axis=1)\n",
    "class_label = -np.ones(No_mutation)\n",
    "class_label[0] = 0\n",
    "group_size = [1]\n",
    "labl = 1\n",
    "\n",
    "for i in range(1, No_mutation):\n",
    "    for j in range(i):\n",
    "        if diff[j, i] == 0:\n",
    "            class_label[i] = class_label[j]\n",
    "            group_size[int(class_label[j])] += 1\n",
    "            break\n",
    "    if class_label[i] == -1:\n",
    "        class_label[i] = labl\n",
    "        labl += 1\n",
    "        group_size.append(1)\n",
    "\n",
    "# quality control\n",
    "least_mut = np.ceil(0.05 * No_mutation)\n",
    "tmp_size = np.min(np.array(group_size)[np.array(group_size) > 0])\n",
    "tmp_grp = np.where(group_size == tmp_size)\n",
    "refine = False\n",
    "if tmp_size < least_mut:\n",
    "    refine = True\n",
    "\n",
    "while refine:\n",
    "    refine = False\n",
    "    tmp_col = np.where(class_label == tmp_grp[0][0])[0]\n",
    "    for i in range(len(tmp_col)):\n",
    "        if tmp_col[i] != 0 and tmp_col[i] != No_mutation - 1:\n",
    "            tmp_diff = np.abs(np.append(np.append(diff[0:tmp_col[i], tmp_col[i]].T.ravel(), 100),\n",
    "                                        diff[tmp_col[i], (tmp_col[i] + 1):No_mutation].ravel()))\n",
    "            tmp_diff[tmp_col] += 100\n",
    "            diff[0:tmp_col[i], tmp_col[i]] = tmp_diff[0:tmp_col[i]]\n",
    "            diff[tmp_col[i], (tmp_col[i] + 1):No_mutation] = tmp_diff[(tmp_col[i] + 1):No_mutation]\n",
    "        elif tmp_col[i] == 0:\n",
    "            tmp_diff = np.append(100, diff[0, 1:No_mutation])\n",
    "            tmp_diff[tmp_col] += 100\n",
    "            diff[0, 1:No_mutation] = tmp_diff[1:No_mutation]\n",
    "        else:\n",
    "            tmp_diff = np.append(diff[0:(No_mutation - 1), No_mutation - 1], 100)\n",
    "            tmp_diff[tmp_col] += 100\n",
    "            diff[0:(No_mutation - 1), No_mutation - 1] = tmp_diff[0:(No_mutation - 1)]\n",
    "        ind = tmp_diff.argmin()\n",
    "        group_size[class_label.astype(np.int64, copy=False)[tmp_col[i]]] -= 1\n",
    "        class_label[tmp_col[i]] = class_label[ind]\n",
    "        group_size[class_label.astype(np.int64, copy=False)[tmp_col[i]]] += 1\n",
    "    tmp_size = np.min(np.array(group_size)[np.array(group_size) > 0])\n",
    "    tmp_grp = np.where(group_size == tmp_size)\n",
    "    refine = False\n",
    "    if tmp_size < least_mut:\n",
    "        refine = True\n",
    "\n",
    "labels = np.unique(class_label)\n",
    "phi_out = np.zeros((len(labels), M))\n",
    "for i in range(len(labels)):\n",
    "    ind = np.where(class_label == labels[i])[0]\n",
    "    class_label[ind] = i\n",
    "    phi_out[i, :] = np.sum(phi_hat[ind, : ] * n[ind, : ], axis=0) / np.sum(n[ind, : ], axis=0)\n",
    "\n",
    "if len(labels) > 1:\n",
    "    sort_phi = sort_by_2norm(phi_out)\n",
    "    phi_diff = sort_phi[1:, :] - sort_phi[:-1, :]\n",
    "    min_ind, min_val = find_min_row_by_2norm(phi_diff)\n",
    "    while np.linalg.norm(min_val) < least_diff:\n",
    "        combine_ind = np.where(phi_out == sort_phi[min_ind, ])[0]\n",
    "        combine_to_ind = np.where(phi_out == sort_phi[min_ind + 1, ])[0]\n",
    "        class_label[class_label == combine_ind] = combine_to_ind\n",
    "        labels = np.unique(class_label)\n",
    "        phi_out = np.zeros((len(labels), M))\n",
    "        for i in range(len(labels)):\n",
    "            ind = np.where(class_label == labels[i])[0]\n",
    "            class_label[ind] = i\n",
    "            phi_out[i] = np.sum(phi_hat[ind, : ] * n[ind, : ], axis=0) / np.sum(n[ind, : ], axis=0)\n",
    "        if len(labels) == 1:\n",
    "            break\n",
    "        else:\n",
    "            sort_phi = sort_by_2norm(phi_out)\n",
    "            phi_diff = sort_phi[1:, :] - sort_phi[:-1, :]\n",
    "            min_ind, min_val = find_min_row_by_2norm(phi_diff)\n",
    "phi_res = np.zeros((No_mutation, M))\n",
    "for lab in range(np.shape(phi_out)[0]):\n",
    "    phi_res[class_label == lab, ] = phi_out[lab, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfae2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c99e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d114746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
